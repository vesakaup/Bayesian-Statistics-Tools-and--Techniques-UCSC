---
title: "Forecasting Finland GDP Growth (Bayesian Regression)"
author: "Vesa Kauppinen"
date: "18 8 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The purpose of this study is to experiment with Bayesian statistical tools and models, while trying forecast Finland GDP growth. The data used was downloaded on  3 May 2020 from [stat.fi](http://pxnet2.stat.fi/PXWeb/pxweb/fi/StatFin/StatFin__kan__vtp/statfin_vtp_pxt_129i.px/), and consists of GDP time series from 1975-2018.



```{r, echo=FALSE, message=FALSE, include = FALSE}
Sys.setlocale("LC_ALL","English")
library(quantmod)
library(reshape2)
library(ggplot2)
library(dplyr)
FIGDP <- read.csv("FIGDP.csv", sep=";")

```

## Overview of the Data

Below is time series plot of Finland GDP.

```{r , echo=FALSE}
plot(FIGDP, type="line", ylab="GDP millon EUR")
lines(FIGDP, type="line",ylab="GDP millon EUR")
```

Next the yearly GDP growth is plotted.
```{r difflog, echo=FALSE}
Y<-data.frame(diff(log(FIGDP[,2])))
Date1 <- data.frame(FIGDP[-1,1]) 
plotz<-data.frame(date=Date1, Y =Y)
plot(plotz[,1],plotz[,2])
lines(plotz[,1],plotz[,2])
```


Our model will be of following form:


$$Y = a + B{_1}Y_{t-1}+B{_2}Y_{t-2} + Îµ$$ 


We choose the number of lags p to be 2. The choice is quite arbitrary; there are some formal tests available to choose the optimal number of lags but that is now out of scope of this exercise. 


```{r, echo=TRUE}
p = 2
T1 = nrow(Y)
```

We implement two functions to get an n\*n matrix with the coefficients along the top row and an (n-1)*(n-1) identity matrix below this.

```{r regmatrix, echo=TRUE}
regression_matrix  <- function(data,p,constant){
        nrow <- as.numeric(dim(data)[1])
        nvar <- as.numeric(dim(data)[2])
        
        Y1 <- as.matrix(data, ncol = nvar)
        X <- embed(Y1, p+1)
        X <- X[,(nvar+1):ncol(X)]
        if(constant == TRUE){
                X <-cbind(rep(1,(nrow-p)),X)
        }
        Y = matrix(Y1[(p+1):nrow(Y1),])
        nvar2 = ncol(X)
        return = list(Y=Y,X=X,nvar2=nvar2,nrow=nrow) 
}
```

```{r armatrix, echo=TRUE}
ar_companion_matrix <- function(beta){
        #check if beta is a matrix
        if (is.matrix(beta) == FALSE){
                stop('error: beta needs to be a matrix')
        }
        # dont include constant
        k = nrow(beta) - 1
        FF <- matrix(0, nrow = k, ncol = k)
        
        #insert identity matrix
        FF[2:k, 1:(k-1)] <- diag(1, nrow = k-1, ncol = k-1)
        
        temp <- t(beta[2:(k+1), 1:1])
        #state space companion form
        #Insert coeffcients along top row
        FF[1:1,1:k] <- temp
        return(FF)
}
```

Next we use the regression matrix function and extract the matrices and number of rows from the results list, and set up priors for Bayesian analysis. We set normal priors for our beta coefficients with mean = 0 and variance = 1. For the variance parameter we have set an inverse gamma prior (congugate prior).


```{r , echo=TRUE}

results = list()
results <- regression_matrix(Y, p, TRUE)
X <- results$X
Y <- results$Y
nrow <- results$nrow
nvar <- results$nvar


# Initialise Priors
B0 <- c(rep(0, nvar))
B <- as.matrix(B0, nrow = 1, ncol = nvar)
sigma0 <- diag(1,nvar)
T0 = 1 # prior degrees of freedom
D0 = 0.1 # prior scale (theta0)
# initial value for variance
sigma2 = 1 
```

Next we set our forecast horizon and initialize matrices to store the results. Matrix called *out* is created to store all of the draws.

```{r , echo=TRUE}
reps = 15000
burn = 4000
horizon = 14
out = matrix(0, nrow = reps, ncol = nvar + 1)
colnames(out) <- c("constant", "beta1","beta2", "sigma")
out1 <- matrix(0, nrow = reps, ncol = horizon)
```

## Gibbs sampling

Next we implement the Gibbs sampling routine.

```{r , echo=TRUE}
gibbs_sampler <- function(X,Y,B0,sigma0,sigma2,theta0,D0,reps,out,out1){
for(i in 1:reps){
    if (i %% 1000 == 0){
    print(sprintf("Iteration: %d", i))
        }
    M = solve(solve(sigma0) + as.numeric(1/sigma2) * t(X) %*% X) %*%
        (solve(sigma0) %*% B0 + as.numeric(1/sigma2) * t(X) %*% Y)
    
    V = solve(solve(sigma0) + as.numeric(1/sigma2) * t(X) %*% X)
    
    chck = -1
    while(chck < 0){   # check for stability
        
        B <- M + t(rnorm(p+1) %*% chol(V))
        
        # Check : not stationary for 3 lags
        b = ar_companion_matrix(B)
        ee <- max(sapply(eigen(b)$values,abs))
        if( ee<=1){
            chck=1
        }
    }
    # compute residuals
    resids <- Y- X%*%B
    T2 = T0 + T1
    D1 = D0 + t(resids) %*% resids
    
    # keeps samples after burn period
    out[i,] <- t(matrix(c(t(B),sigma2)))
    
    
    #draw from Inverse Gamma
    z0 = rnorm(T1,1)
    z0z0 = t(z0) %*% z0
    sigma2 = D1/z0z0
    
    # keeps samples after burn period
    out[i,] <- t(matrix(c(t(B),sigma2)))
    
    # compute 2 year forecasts
    yhat = rep(0,horizon)
    end = as.numeric(length(Y))
    yhat[1:2] = Y[(end-1):end,]
    cfactor = sqrt(sigma2)
    X_mat = c(1,rep(0,p))
for(m in (p+1):horizon){
            for (lag in 1:p){
            #create X matrix with p lags
                X_mat[(lag+1)] = yhat[m-lag]
    }
            # Use X matrix to forecast yhat
            yhat[m] = X_mat %*% B + rnorm(1) * cfactor
    }
    
out1[i,] <- yhat
}
    return = list(out,out1)
    }
results1 <- gibbs_sampler(X,Y,B0,sigma0,sigma2,T0,D0,reps,out,out1)
# burn first 4000
coef <- results1[[1]][(burn+1):reps,]
forecasts <- results1[[2]][(burn+1):reps,]
```

Next we extract the coefficients that are needed to which correspond to the columns of the coefficient matrix. Each row gives the value of the parameter for each draw of the Gibbs sampler. Calculating the mean of each of the variables gives us an approximation of the posterior mean of the distribution for each coefficient. The posterior distributions are plotted.

```{r , echo=TRUE}
const <- mean(coef[,1])
beta1 <- mean(coef[,2])
beta2 <- mean(coef[,3])
sigma <- mean(coef[,4])
qplot(coef[,1], geom = "histogram", bins = 45, main = 'Distribution of Constant',
      colour='black')
qplot(coef[,2], geom = "histogram", bins = 45,main = 'Distribution of Beta1',
      colour='black')
qplot(coef[,3], geom = "histogram", bins = 45,main = 'Distribution of Beta2',
      colour='black')
qplot(coef[,4], geom = "histogram", bins = 45,main = 'Distribution of Sigma', colour='black')
```

## Plotting the forecast

Below is the forecast for year over over GDP growth with intervals highlighting the uncertainty of the forecasts (the true parameter being within this range with 95 % probability). 16 and 84 percentiles are used as credible intervals.

```{r , echo=TRUE, message = FALSE}
library(matrixStats); library(ggplot2); library(reshape2)
#quantiles for all data points, makes plotting easier
post_means <- colMeans(coef)
forecasts_m <- as.matrix(colMeans(forecasts))
#Creating error bands/credible intervals around our forecasts
error_bands <- colQuantiles(forecasts,prob = c(0.16,0.84))
Y_temp = cbind(Y,Y)
error_bands <- rbind(Y_temp, error_bands[3:dim(error_bands)[1],])
all <- as.matrix(c(Y[1:(length(Y)-2)],forecasts_m))
forecasts.mat <- cbind.data.frame(error_bands[,1],all, error_bands[,2])
names(forecasts.mat) <- c('lower', 'mean', 'upper')
# create date vector for plotting
Date <- seq(as.Date('1975/01/01'), by = 'year', length.out = dim(forecasts.mat)[1])

data.plot <- cbind.data.frame(Date, forecasts.mat)
data_subset <- data.plot[20:54,]
data_fore <- data.plot[44:54,]
ggplot(data_subset, aes(x = Date, y = mean)) +
  geom_line(colour = 'blue', lwd = 1.2) +
  geom_ribbon(data = data_fore,
  aes(ymin = lower, ymax = upper , colour = "bands", alpha = 0.2))
```

In conclusion, the model forecasts mean prediction of growth of about 5 percent and negative growth periods to be likely below 2 percent. This seems unrealistically optimistic, and obviously the model fails to predict the effect of Covid19 pandemic.
